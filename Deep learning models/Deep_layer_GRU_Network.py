# -*- coding: utf-8 -*-
""""Hinglish" Language - Modeling a MessyCode-Mixed Language.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CYAvCp2GD-3anA2686ZTEja05gqTXCD_
"""

import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv("/content/drive/My Drive/NLP_Project/data/IIITH_Codemixed2.csv",names = ["Speech", "Labels"])
df = pd.read_csv("/content/drive/My Drive/NLP_Project/data/train_new.txt",sep = "\t",names = ["Speech", "Labels"])
df_new = pd.concat([df,df1], ignore_index=True)
df = df_new
df["Labels"] = df["Labels"].astype("int")
df.head()
label_counts = df["Labels"].value_counts().tolist()
labels = df["Labels"].value_counts().index.tolist()
labels = list(map(int, labels))
print(label_counts)

plt.bar(labels, label_counts)



#Data Cleaning
#Removing all the punctuation marks and converting to lowercase
import re

def clean_str(string):
    """Tokenization/string cleaning for all datasets except for SST.

    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
    """
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r"\s{2,}", " ", string)
    string = re.sub(r"[.,#!$%&;:{}=_`~()/\\]", "", string)
    return string.strip().lower()

df['Speech'] = df['Speech'].astype(str).map(clean_str,na_action=None)


df = df.sample(frac=1).reset_index(drop=True)


labels = df["Labels"].values

text = df['Speech']

sentences1 = text.apply(lambda x: x.split())
#storing the original dataset without syllable in an object
sent_without_syllable = sentences1



sentences2_without_syll = [d for d in sent_without_syllable]

print(sent_without_syllable)

print(sentences2_without_syll)

from gensim.models import FastText
model_without_syll = FastText(size=200, window=50, min_count=1)
model_without_syll.build_vocab(sentences=sentences2_without_syll)
model_without_syll.train(sentences=sentences2_without_syll, total_examples=len(sentences2_without_syll), epochs=10)
words = list(model_without_syll.wv.vocab)
print(words)

similar_words = model_without_syll.most_similar('chutiya')	
print(similar_words)

model_without_syll.save("/content/drive/My Drive/NLP_Project/model_without_syll_fasttext.model")

from gensim.models.word2vec import Word2Vec
from multiprocessing import cpu_count
import logging
print("Training model")
model_without_syll = Word2Vec(sentences2_without_syll, min_count = 1,size = 200,  workers=cpu_count(),max_vocab_size = None, iter = 10, sg =1, hs =1)
 

words = list(model_without_syll.wv.vocab)
print(words)


model_without_syll.save("/content/drive/My Drive/NLP_Project/model_without_syll.model")




similar_words = model_without_syll.most_similar('madarchod')	
print(similar_words)

from   keras import initializers, regularizers, constraints, callbacks, optimizers
from   keras.layers import Conv1D, Embedding, GlobalMaxPooling1D, Concatenate, Input, Dense
from   keras.models import Sequential, Model
from   keras.preprocessing.sequence import pad_sequences
from   keras.preprocessing.text import Tokenizer
from   sklearn.model_selection import train_test_split
import tensorflow as tf
import numpy as np



sentences_train,sentences_test,y_train,y_test = train_test_split(
                                                sentences1, labels,  
                                                test_size=0.2 
                                                )




MAX_WORDS = 15000 #number of unique words to consider for tokenizer, rest all words are treated as out of vocabulary tokens

tokenizer = Tokenizer(num_words=MAX_WORDS, char_level=False, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences1)

X_train = tokenizer.texts_to_sequences(sentences_train)
X_test = tokenizer.texts_to_sequences(sentences_test)
word_index  = tokenizer.word_index

from tensorflow.keras.preprocessing.sequence import pad_sequences

len_max = 40 #max length of a sentence in dataframe is consisting of 124 words
X_train  = pad_sequences(X_train ,padding="post",  truncating = "post", maxlen = len_max)
X_test  = pad_sequences(X_test ,padding="post",  truncating = "post", maxlen = len_max)

X_test[:3,:]

print(word_index)

EMBEDDING_DIM = 200
vocabulary_size = len(word_index) + 1
embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))


print("Start embedding matrix creation")
for word, i in word_index.items():
    if i < MAX_WORDS:
        try:
            embedding_vector = model_without_syll[word]
            embedding_matrix[i] = embedding_vector
        except KeyError:
            embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)



embedding_matrix[:5,180:190]

from sklearn import preprocessing
import keras.utils.np_utils

encoder = preprocessing.LabelEncoder()
encoder.fit(y_train)
y_train = encoder.transform(y_train)
y_test  = encoder.transform(y_test)
# convert integers to dummy variables (i.e. one hot encoded)
y_train = keras.utils.np_utils.to_categorical(y_train)
y_test = keras.utils.np_utils.to_categorical(y_test)

#The model architecture is inspired from http://cs230.stanford.edu/projects_fall_2019/reports/26251423.pdf
from keras.layers import LSTM, GRU, Bidirectional, BatchNormalization
from keras.utils import plot_model
print ("Start creating model")
#in the input layer the vectors that will come in will be of the max_length size
input_embedding = Input(shape = (len_max,))

# after there will the embedding layer that will take the tokenized and padded words and will turn them into
# dense words through the word2vec pretrained model
embedding = Embedding(vocabulary_size,
                      EMBEDDING_DIM,
                      weights = [embedding_matrix],
                      trainable = False)(input_embedding)
bilstm1 = Bidirectional(GRU(64, activation='relu',reset_after=False, recurrent_activation='sigmoid', return_sequences=True, recurrent_dropout=0.2))(embedding)
bilstm2 = Bidirectional(GRU(64, activation='relu', reset_after=False, recurrent_activation='sigmoid', return_sequences=True, recurrent_dropout=0.2))(bilstm1)
bilstm3 = Bidirectional(GRU(64, activation='relu',reset_after=False, recurrent_activation='sigmoid', return_sequences=True, recurrent_dropout=0.2))(bilstm2)
bilstm4 = Bidirectional(GRU(64, activation='relu',reset_after=False, recurrent_activation='sigmoid', return_sequences=True, recurrent_dropout=0.2))(bilstm3)

Dense_layer = Dense(units = 3, activation = 'softmax', )(bilstm4)

drop = tf.keras.layers.Dropout(.2)(Dense_layer)
pool1 = GlobalMaxPooling1D()(drop)


drop2 = tf.keras.layers.Dropout(.2)(pool1)
output = Dense(units = 3, activation = 'softmax')(drop2)

model_biLSTM = Model(inputs = [input_embedding], outputs = output)


model_biLSTM.compile(loss = 'categorical_crossentropy',
              optimizer = optimizers.Adam(learning_rate= 0.001),
              metrics   = ['accuracy'])


print(model_biLSTM.summary())
plot_model(model_biLSTM, to_file='/content/drive/My Drive/NLP_Project/model_biLSTM_custom.png')

from keras.callbacks import ModelCheckpoint
filepath="weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train,
                                              train_size = 0.85)

history = model_biLSTM.fit(Xtrain, ytrain, validation_data=(Xval, yval),
          epochs=20, batch_size=16, callbacks = callbacks_list)

from google.colab import drive
drive.mount('/content/drive')

from matplotlib import pyplot
# evaluate the model
_, train_acc = model_biLSTM.evaluate(Xtrain, ytrain, verbose=0)
_, test_acc = model_biLSTM.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()

from sklearn.metrics import classification_report
#Precision Recall and F1-Score, metrics for evaluation

y_pred_LSTM = model_biLSTM.predict(X_test, batch_size=64, verbose=1)
y_pred_LSTM = np.argmax(y_pred_LSTM, axis=1)

y_test_LSTM=np.argmax(y_test, axis=1)
#rounded_labels[1]

from sklearn.metrics import classification_report
#Precision Recall and F1-Score, metrics for evaluation

y_pred_LSTM = model_biLSTM.predict(X_test, batch_size=64, verbose=1)
y_pred_LSTM = np.argmax(y_pred_LSTM, axis=1)

y_test_LSTM=np.argmax(y_test, axis=1)
#rounded_labels[1]

print(classification_report(y_test_LSTM, y_pred_LSTM))

model_json = model_biLSTM.to_json()
with open("/content/drive/My Drive/NLP_Project/model_biLSTM_stanford.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model_biLSTM.save_weights("/content/drive/My Drive/NLP_Project/model_biLSTM_stanford.h5")

