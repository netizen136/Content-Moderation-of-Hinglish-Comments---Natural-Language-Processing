# -*- coding: utf-8 -*-
"""Hate_Speech-detection_Code_Mixed_data_for_CNN_LSTM_BiLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZTmozAWsvfBcxlVo4Y6iRoEH9Df9H15F
"""



import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv("/content/drive/My Drive/NLP_Project/data/IIITH_Codemixed2.csv",names = ["Speech", "Labels"])
df1 = pd.read_csv("/content/drive/My Drive/NLP_Project/data/train_new.txt",sep = "\t",names = ["Speech", "Labels"])

df_new = pd.concat([df,df1], ignore_index=True)
df = df_new

df["Labels"] = df["Labels"].astype("int")
df.head()

df.shape

label_counts = df["Labels"].value_counts().tolist()
labels = df["Labels"].value_counts().index.tolist()
labels = list(map(int, labels))
print(label_counts)

plt.bar(labels, label_counts)

#Data Cleaning
#Removing all the punctuation marks and converting to lowercase
import re

def clean_str(string):
    """Tokenization/string cleaning for all datasets except for SST.

    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
    """
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r"\s{2,}", " ", string)
    string = re.sub(r"[.,#!$%&;:{}=_`~()/\\]", "", string)
    return string.strip().lower()

df['Speech'] = df['Speech'].astype(str).map(clean_str,na_action=None)

df.head()

df = df.sample(frac=1).reset_index(drop=True)
df.head()

print(df.Speech.map(lambda x: len(x)).max())

labels = df["Labels"].values

text = df['Speech']

sentences1 = text.apply(lambda x: x.split())

sentences2 = [d for d in sentences1]

print(sentences1.shape)

from gensim.models.word2vec import Word2Vec
from multiprocessing import cpu_count
import logging
print("Training model")
model = Word2Vec(sentences2, min_count = 1,size = 200,  workers=cpu_count(),max_vocab_size = None, iter = 20, sg =1)
#model["rohitsharmawpg"]

words = list(model.wv.vocab)
print(words)

model.save("/content/drive/My Drive/NLP_Project/embed_model")

similar_words = model.most_similar('madarchod')	
print(similar_words)

from   keras import initializers, regularizers, constraints, callbacks, optimizers
from   keras.layers import Conv1D, Embedding, GlobalMaxPooling1D, Concatenate, Input, Dense
from   keras.models import Sequential, Model
from   keras.preprocessing.sequence import pad_sequences
from   keras.preprocessing.text import Tokenizer
from   sklearn.model_selection import train_test_split
import tensorflow as tf
import numpy as np

sentences_train,sentences_test,y_train,y_test = train_test_split(
                                                sentences1, labels,  
                                                test_size=0.25  
                                                )

sentences_train.shape

import io
import json
MAX_WORDS = 17000 #number of unique words to consider for tokenizer, rest all words are treated as out of vocabulary tokens

tokenizer = Tokenizer(num_words=MAX_WORDS,  oov_token="<OOV>")
tokenizer.fit_on_texts(sentences1)

X_train = tokenizer.texts_to_sequences(sentences_train)
X_test = tokenizer.texts_to_sequences(sentences_test)
word_index  = tokenizer.word_index
tokenizer_json = tokenizer.to_json()
with io.open('/content/drive/My Drive/NLP_Project/tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

from tensorflow.keras.preprocessing.sequence import pad_sequences

len_max = 60
X_train  = pad_sequences(X_train ,padding="post",  truncating = "post", maxlen = len_max)
X_test  = pad_sequences(X_test ,padding="post",  truncating = "post", maxlen = len_max)

import numpy as np
EMBEDDING_DIM = 200
vocabulary_size = len(word_index) + 1
embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))
embedding_matrix.shape

print("Start embedding matrix creation")
for word, i in word_index.items():
    if i < MAX_WORDS:
        try:
            embedding_vector = model[word]
            embedding_matrix[i] = embedding_vector
        except KeyError:
            embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)

from sklearn import preprocessing
import keras.utils.np_utils

encoder = preprocessing.LabelEncoder()
encoder.fit(y_train)
y_train = encoder.transform(y_train)
y_test  = encoder.transform(y_test)
# convert integers to dummy variables (i.e. one hot encoded)
y_train = keras.utils.np_utils.to_categorical(y_train)
y_test = keras.utils.np_utils.to_categorical(y_test)

y_train.shape

embedding_matrix.shape

#1. CNN_1D
print ("Start creating model")
#in the input layer the vectors that will come in will be of the max_length size
input_embedding = Input(shape = (len_max,))

# after there will the embedding layer that will take the tokenized and padded words and will turn them into
# dense words through the word2vec pretrained model
embedding = Embedding(vocabulary_size,
                      EMBEDDING_DIM,
                      weights = [embedding_matrix],
                      trainable = False)(input_embedding)

#3 parallel convolution layers that will try to extract features
conv1 = Conv1D(filters = 64, kernel_size = 2, padding = 'same')(embedding)
conv2 = Conv1D(filters = 64, kernel_size = 3, padding = 'same')(embedding)
conv3 = Conv1D(filters = 64, kernel_size = 4, padding = 'same')(embedding)

pool1 = GlobalMaxPooling1D()(conv1)
drop_out1 = tf.keras.layers.Dropout(.5)(pool1)

pool2 = GlobalMaxPooling1D()(conv2)
drop_out2 = tf.keras.layers.Dropout(.5)(pool2)

pool3 = GlobalMaxPooling1D()(conv3)
drop_out3 = tf.keras.layers.Dropout(.5)(pool3)


merged_tensor = Concatenate(axis=1)([drop_out1,drop_out2,drop_out3])
drop_out = tf.keras.layers.Dropout(.5)(merged_tensor)
output = Dense(units = 3, activation = 'softmax')(drop_out)
model_cnn = Model(inputs = [input_embedding], outputs = output)



model_cnn.compile(loss      = 'categorical_crossentropy',
              optimizer = optimizers.Adam(),
              metrics   = ['accuracy'])

print(model_cnn.summary())
print ("End creating model")

from keras.utils import plot_model
plot_model(model_cnn, to_file='/content/drive/My Drive/NLP_Project/cnn1d.png')

Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train,
                                              train_size = 0.85)

history = model_cnn.fit(Xtrain, ytrain, validation_data=(Xval, yval),
          epochs=50, batch_size=32)

from matplotlib import pyplot
# evaluate the model
_, train_acc = model_cnn.evaluate(Xtrain, ytrain, verbose=0)
_, test_acc = model_cnn.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()

from sklearn.metrics import classification_report

#Precision Recall and F1-Score, metrics for evaluation

y_pred = model_cnn.predict(X_test, batch_size=64, verbose=1)
y_pred = np.argmax(y_pred, axis=1)

y_test_cnn=np.argmax(y_test, axis=1)
#rounded_labels[1]

print(classification_report(y_test_cnn, y_pred))

model_json = model_cnn.to_json()
with open("/content/drive/My Drive/NLP_Project/model_cnn.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model_cnn.save_weights("/content/drive/My Drive/NLP_Project/model_cnn.h5")

#LSTM
from keras.layers import LSTM
print ("Start creating model")
#in the input layer the vectors that will come in will be of the max_length size
input_embedding = Input(shape = (len_max,))

# after there will the embedding layer that will take the tokenized and padded words and will turn them into
# dense words through the word2vec pretrained model
embedding = Embedding(vocabulary_size,
                      EMBEDDING_DIM,
                      weights = [embedding_matrix],
                      trainable = False)(input_embedding)
lstm = LSTM(100, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True, recurrent_dropout=0.2)(embedding)
pool1 = GlobalMaxPooling1D()(lstm)

output = Dense(units = 3, activation = 'softmax')(pool1)
model_LSTM = Model(inputs = [input_embedding], outputs = output)


model_LSTM.compile(loss      = 'categorical_crossentropy',
              optimizer = optimizers.Adam(),
              metrics   = ['accuracy'])

print(model_LSTM.summary())
print ("End creating model")

from keras.utils import plot_model
plot_model(model_LSTM, to_file='/content/drive/My Drive/NLP_Project/model_LSTM.png')

history = model_LSTM.fit(Xtrain, ytrain, validation_data=(Xval, yval),
          epochs=20, batch_size=64)

# evaluate the model
_, train_acc = model_LSTM.evaluate(Xtrain, ytrain, verbose=0)
_, test_acc = model_LSTM.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()


#Precision Recall and F1-Score, metrics for evaluation

y_pred_LSTM = model_LSTM.predict(X_test, batch_size=64, verbose=1)
y_pred_LSTM = np.argmax(y_pred_LSTM, axis=1)

y_test_LSTM=np.argmax(y_test, axis=1)
#rounded_labels[1]

print(classification_report(y_test_LSTM, y_pred_LSTM))

model_json = model_LSTM.to_json()
with open("/content/drive/My Drive/NLP_Project/model_LSTM.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model_LSTM.save_weights("/content/drive/My Drive/NLP_Project/model_LSTM.h5")

#BiLSTM
from keras.layers import Bidirectional
print ("Start creating model")
#in the input layer the vectors that will come in will be of the max_length size
input_embedding = Input(shape = (len_max,))

# after there will the embedding layer that will take the tokenized and padded words and will turn them into
# dense words through the word2vec pretrained model
embedding = Embedding(vocabulary_size,
                      EMBEDDING_DIM,
                      weights = [embedding_matrix],
                      trainable = False)(input_embedding)

bilstm = Bidirectional(LSTM(100, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True, recurrent_dropout=0.2))(embedding)
pool1 = GlobalMaxPooling1D()(bilstm)


output = Dense(units = 3, activation = 'softmax')(pool1)
model_biLSTM = Model(inputs = [input_embedding], outputs = output)


model_biLSTM.compile(loss = 'categorical_crossentropy',
              optimizer = optimizers.Adam(),
              metrics   = ['accuracy'])


print(model_biLSTM.summary())
plot_model(model_biLSTM, to_file='/content/drive/My Drive/NLP_Project/model_biLSTM.png')

history = model_biLSTM.fit(Xtrain, ytrain, validation_data=(Xval, yval),
          epochs=20, batch_size=64)

# evaluate the model
_, train_acc = model_biLSTM.evaluate(Xtrain, ytrain, verbose=0)
_, test_acc = model_biLSTM.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()


#Precision Recall and F1-Score, metrics for evaluation

y_pred_biLSTM = model_biLSTM.predict(X_test, batch_size=64, verbose=1)
y_pred_biLSTM = np.argmax(y_pred_biLSTM, axis=1)

y_test_biLSTM=np.argmax(y_test, axis=1)


print(classification_report(y_test_biLSTM, y_pred_biLSTM))

model_json = model_biLSTM.to_json()
with open("/content/drive/My Drive/NLP_Project/model_biLSTM.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model_biLSTM.save_weights("/content/drive/My Drive/NLP_Project/model_biLSTM.h5")

